{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, im going to write the script that is in charge of , based on values of pi and beta, be able to select the snps that are going to be contributing to the phenotype and calculate their effect sizes \n",
    "and then calculate the optima phenotypes based on a temperature gradient and the contributing snps\n",
    "and then finally i will also generate the fasta file used as reference for slim, basically generated from the vcf file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import allel\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params for annotating vcf file with sc\n",
    "chr_number = 5\n",
    "vcf_file = 'chr5_grenenet_subset.vcf'\n",
    "pi =  0.01\n",
    "beta = 4\n",
    "\n",
    "#params for calculating optima \n",
    "grenenet_1001g_ecotypes = 'ecotypes_grenenet_1001g.txt'\n",
    "path_worldclim_ecotypesdata = '/Users/tbellagio/safedata/ath_evo/grenephase1/data/worldclim_ecotypesdata.csv'\n",
    "bed_sc = 'selection_coef_chr5_subset.bed'\n",
    "\n",
    "## params for gen of fasta from vcf \n",
    "path_vcf = '' \n",
    "path_fasta = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pi can be interpreted as the probability of a bernoulli trial for each snps contributing to the qtl\n",
    "because there are many snps, this can be thought as a binomial with n trials (number of positions in the chromosome) and each with probability pi. \n",
    "Becuase the expected value or mean of a binomial is n*p we can just multiply those values to get to the expected value of snps contributing to the qtl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_p = allel.read_vcf(vcf_file, fields='variants/POS')\n",
    "## get all the pos from the vcf file\n",
    "file_p = allel.read_vcf(vcf_file, fields='variants/POS')\n",
    "pos = file_p['variants/POS']\n",
    "n_pos = len(pos)\n",
    "n_pos_contr =round(pi*n_pos)\n",
    "## randomly select the positions that are going to contribute \n",
    "pos_contr = random.sample(pos.tolist(), k=n_pos_contr)\n",
    "## now im gonna sample the number of nps contributing from a normal distribution with mean 0 and sd defined by beta \n",
    "mu, sigma = 0,beta # mean and standard deviation\n",
    "effect_sizes = np.random.normal(mu, sigma, n_pos_contr)\n",
    "effect_sizes = np.round(effect_sizes, 4)\n",
    "## now i have to create a bed file to add this infromation to the vcf file \n",
    "## create dataframe of the contributing pos \n",
    "\n",
    "\n",
    "## to caluclate the positions FROM in a bed file, the fist value is not included\n",
    "positions_from = [i-1 for i in pos]\n",
    "# Define the positions and values\n",
    "chromosome = [chr_number] * len(pos)\n",
    "\n",
    "# Create a DataFrame from the positions and values\n",
    "all_pos = pd.DataFrame({'chromosome':chromosome,'positions_from': positions_from, 'positions_to': pos})\n",
    "contrib_pos = pd.DataFrame({'positions_to': pos_contr, 'sel_coef':effect_sizes})\n",
    "\n",
    "# merge and fillna with 0 since that is the selection coefficient for all the non contributing snps \n",
    "all_pos = all_pos.merge(contrib_pos, left_on= 'positions_to', right_on= 'positions_to' ,how='left').fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a BED file\n",
    "all_pos.to_csv(bed_sc, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optima caclculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecotypes_grenenet_1001g = pd.read_csv(grenenet_1001g_ecotypes).columns.astype(int)\n",
    "ecotypes_temp = pd.read_csv(path_worldclim_ecotypesdata, usecols=['ecotypeid', 'bio1'])\n",
    "#BIO1 = Annual Mean Temperature\n",
    "## filter by the ones coming from the 1001 genomes\n",
    "ecotypes_temp = ecotypes_temp[ecotypes_temp['ecotypeid'].isin(ecotypes_grenenet_1001g)]\n",
    "## get the total number of position in the vcf file that i am working with, so then i can calculate the selection coefficients \n",
    "max_t = ecotypes_temp['bio1'].max()\n",
    "min_t = ecotypes_temp['bio1'].min()\n",
    "\n",
    "samples = pd.DataFrame(columns = ecotypes_temp.columns)\n",
    "\n",
    "num_ranges = 5\n",
    "\n",
    "# calculate the range width\n",
    "range_width = (max_t - min_t) / num_ranges\n",
    "\n",
    "# create the ranges\n",
    "ranges = [(min_t + i*range_width, min_t + (i+1)*range_width) for i in range(num_ranges)]\n",
    "\n",
    "# sample across the ranges\n",
    "for r in ranges:\n",
    "    r_ages = ecotypes_temp[( ecotypes_temp['bio1'] >= r[0]) & ( ecotypes_temp['bio1'] <= 1+r[1])]\n",
    "    samples = pd.concat([samples, r_ages.sample(7, replace=True)])\n",
    "## and then to complete i will drop one randomly \n",
    "drop_index = np.random.choice(samples.index)\n",
    "samples = samples.drop(drop_index)\n",
    "\n",
    "selection_coef = pd.read_csv(bed_sc, sep = '\\t',header = None)[3]\n",
    "\n",
    "vcf = allel.read_vcf(vcf_file)\n",
    "phenotypes = []\n",
    "for ecotype in samples['ecotypeid']:\n",
    "    vcf_ecotype = allel.read_vcf(vcf_file, samples=[str(ecotype)])\n",
    "    ## for each position this is the number of alterantive variants \n",
    "    alt_alleles_per_pos = vcf_ecotype['calldata/GT'].sum(axis=2)\n",
    "    gen_effectsize = np.multiply(alt_alleles_per_pos.flatten(), np.array(selection_coef))  ## select coef are actually effect sizes\n",
    "    phenotypes.append(gen_effectsize.sum())\n",
    "samples['phenotype'] = phenotypes\n",
    "samples = samples.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write optimas\n",
    "for i in range(0,34):\n",
    "    optima = pd.concat([samples[samples.index == i]]*12, ignore_index=True)\n",
    "    optima.to_csv(f'optima_files/optima{i}.csv')\n",
    "    optima['phenotype'].to_csv(f'optima_files/optima{i}_slim.txt', header=None, sep='\\n', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generation of reference fasta from vcf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def from_vcf_to_fasta(vcf_file_name, path_vcf, path_fasta):\n",
    "vcf = allel.read_vcf(path_vcf + vcf_file)\n",
    "seq_firstchr = ''\n",
    "for i in range(0, len(vcf['variants/REF'])):\n",
    "    seq_firstchr += vcf['variants/REF'][i]\n",
    "\n",
    "start = vcf['variants/POS'][0]\n",
    "end = vcf['variants/POS'][-1]\n",
    "chro = vcf['variants/CHROM'][0]\n",
    "with open(path_fasta + vcf_file[:-4] + '.fasta', 'a') as f:\n",
    "    f.write(f'>{chro}:{start}-{end}\\n')\n",
    "    f.write(seq_firstchr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
