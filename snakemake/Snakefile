# =================================================================================================
#     Dependencies
# =================================================================================================


configfile: "config.yaml"


replicates = list(range(0, config["replicates"]))
optima = list(range(0, config["optima"]))

## this rule runs a python script that will generate the bed file containing the contributing loci and their effect sizes based on values of beta dn alpha
## the bed file will be then used to annotate a vcf file that will be used by SliM to run the simulations


rule all:
    input:
        expand(
            "results/arq_pi{pi}_beta{beta}/allele_counts.csv",
            pi=config["pi"],
            beta=config["beta"],
        ),
        expand(
            "results/arq_pi{pi}_beta{beta}/pop_sizes.csv",
            pi=config["pi"],
            beta=config["beta"],
        ),


rule run_python_script:
    input:
        base_vcf=config["base_vcf"],
        optimum_ecotypes=config["optimum_ecotypes"],
        base_fasta=config["base_fasta"],
    output:
        bed_sc="results/arq_pi{pi}_beta{beta}/selection_coef.bed",
        optima="results/arq_pi{pi}_beta{beta}/optima_files/optima.csv",
        slim="results/arq_pi{pi}_beta{beta}/optima_files/optima_slim.txt",
    params:
        pi=lambda wildcards: str(wildcards.pi),
        beta=lambda wildcards: str(wildcards.beta),
        chr_number=config["chr_number"],
    resources:
        mem_mb=61440,
    conda:
        "envs/base_env.yaml"
    script:
        "scripts/pi_beta_calc.py"


rule run_bash_script:
    input:
        vcf_file=config["base_vcf"],
        bed_sc="results/arq_pi{pi}_beta{beta}/selection_coef.bed",
    output:
        "results/arq_pi{pi}_beta{beta}/annotated.vcf",
    threads: 10
    log:
        out="log/pi{pi}_beta{beta}_run_bash_script_stdout.log",
        err="log/pi{pi}_beta{beta}_run_bash_script_stderr.err",
    resources:
        mem_mb=61440,
    conda:
        "envs/base_env.yaml"
    script:
        "scripts/annotate.sh"


rule run_slim_script:
    input:
        fasta=config["base_fasta"],
        vcf="results/arq_pi{pi}_beta{beta}/annotated.vcf",
        optima_slim="results/arq_pi{pi}_beta{beta}/optima_files/optima_slim.txt",
    output: 
        "results/arq_pi{pi}_beta{beta}/optima{optima}/subp{replicates}_slim_output.txt",
    params:
        optima=lambda wildcards: str(wildcards.optima),
        initial_pop=config["initial_pop"],
    resources:
        mem_mb=102400,
    conda:
        "envs/base_env.yaml"
    script:
        "scripts/slim.sh"


rule create_allele_counts_pool_size_tables:
    input:
        expand(
            "results/arq_pi{{pi}}_beta{{beta}}/optima{optima}/subp{replicates}_slim_output.txt",
            optima=optima,    
            replicates=replicates,    
        ),
    output:
        allele_counts = "results/arq_pi{pi}_beta{beta}/allele_counts.csv",
        pool_sizes = "results/arq_pi{pi}_beta{beta}/pop_sizes.csv",
    params:
        pi=lambda wildcards: str(wildcards.pi),
        beta=lambda wildcards: str(wildcards.beta),
    resources:
        mem_mb=61440,
    conda:
        "envs/base_env.yaml"
    script:
        "scripts/allele_counts_pool_sizes.py"


#        "results/arq_pi{{pi}}_beta{{beta}}/optima{{optima}}/pop_sizes.csv",
#    log:
#        out="log/arq_pi{pi}_beta{beta}_optima{optima}_stdout.log",
#        err="log/arq_pi{pi}_beta{beta}_optima{optima}_stderr.err",

# rule ld_pruning:
#     input:
#         vcf_file = config['base_vcf'],
#         maf= config[maf]
#         corr_ld= config[corr_ld]
#     output:
#         "data/chr5_grenenet_filteredmaf.prune.in"
# resources:
#     mem_mb=61440
#     log:
#         out = "log/pi{pi}_beta{beta}_run_bash_script_stdout.log",
#         err = "log/pi{pi}_beta{beta}_run_bash_script_stderr.err"
#     conda:
#         'envs/base_env.yaml'
#     script:
#         "ld_pruning.sh"

# rule apply_ld_pruning:
#     input:
#         allele_counts_table = 'results/arq_pi{pi}_beta{beta}/simulation_results/allele_counts.csv',
#         snps_to_keep = 'data/chr5_grenenet_filteredmaf.prune.in'
#     output:
#         allele_counts_table_filtered = 'results/arq_pi{pi}_beta{beta}/simulation_results/allele_counts_filtered.csv',
#     resources:
#         mem_mb=61440
#     log:
#         out = "log/pi{pi}_beta{beta}_run_bash_script_stdout.log",
#         err = "log/pi{pi}_beta{beta}_run_bash_script_stderr.err"
#     conda:
#         'envs/base_env.yaml'
#     script:
#         "apply_ld_pruning.sh"


## for output vcf
# expand(
#    "results/arq_pi{{pi}}_beta{{beta}}/vcf_files/optima{{optima}}/subp{replicates}.vcf",
#    replicates = range(0,12)
# )


## backlog
## this was the begining thing


# TAIR10_chr_all.fas | awk '/^>Chr5 /{p=1}p' > chr5.fasta
# ## then in the python script this file will be modified so it is int he slim format

# ##### phase 1 #######################################
# ## filter ch5 and grenenet ecotypes from the big file

# bcftools view 1001_genomes_snps_missing0.8_merged_imputed_biallelic_named.vcf.gz --regions 5 -o chr5.vcf.gz

# bcftools view -S ecotypes_grenenet.txt chr5.vcf.gz -o chr5_grenenet.vcf
