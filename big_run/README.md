so before running the pipeline i had to load: 

## for cluster
# module load Conda/3.7 
# module load BCFtools/1.10.2 
# module load SLiM/4.0.1 


# and then so that the conda env qith the packages needed is in the cluster 
conda env create -f snakes_check.yml


####### run the pipeline ############################
######################################################
## files needed to run it 
# pi_beta_calc.py
# chr5_grenenet.vcf 
# optimum_ecotypes.csv -> generated by optimum_ecotypes_nopheno.py 
# chr5.fasta -> generated by fasta_correction_for_slim.py 
# bash_slim.sh   # iteration through the arabidopsis_evolve.slim file for 12 replicates
# arabidopsis_evolve.slim


######### scripts that dont ened to be run every time #########################################
### in case you want to re run the sample of ecotypes across and environmental gradient: 
# python optimum_ecotypes_nopheno.py 
## this script needs 'ecotypes_grenenet_1001g.txt' and 'worldclim_ecotypesdata.csv' 
# and right now runs for 32 environments 
# output: optimum_ecotypes.csv

### this script is for correcting a fasta file directly download from tair or similar since slim only accept A,C,G,T in their fasta files 
# python fasta_correction_for_slim.py 
# output: chr5.fasta
######### scripts that dont ened to be run every time #########################################

### pi is 0.0001 since length_chr5 = 1702174 * 0.0001 = 170
# python pi_beta_optima_fasta.py pi beta
echo PHASE 1 based on beta and pi calculate optimum phenotypes based on selected snps and their effect sizes 
python pi_beta_calc.py 0.0001 5

##### phase 3 #######################################
############## annotate vcf with sc 
echo PHASE 2 annotate vcf with effect sizes 
## first
bgzip -f selection_coef_chr5.bed 
bgzip -f chr5_grenenet.vcf 
bcftools tabix -f selection_coef_chr5.bed.gz 
bcftools tabix -f chr5_grenenet.vcf.gz 
# -f force in case this is past the first run of the pipeline and these files already exist 

## and then annotate
bcftools annotate \
  -a selection_coef_chr5.bed.gz \
  -c CHROM,FROM,TO,S \
  -h <(echo '##INFO=<ID=S,Number=.,Type=Float,Description="Selection Coefficient">') \
  chr5_grenenet.vcf.gz \
  -o chr5_grenenet_ann.vcf

#decompress so it can be usen by the python when run this script again 
bgzip -d chr5_grenenet.vcf.gz 

echo PHASE 4 amplify number of ecotypes to simulate starting seedmix
##### phase 4 #######################################
###### amplify number of ecotypes #########################################################
bcftools query -l chr5_grenenet_ann.vcf | wc -l ## now only 225

bgzip chr5_grenenet_ann.vcf
bcftools tabix -f chr5_grenenet_ann.vcf.gz 

bcftools merge --threads 4 --force-samples chr5_grenenet_ann.vcf.gz chr5_grenenet_ann.vcf.gz chr5_grenenet_ann.vcf.gz chr5_grenenet_ann.vcf.gz -o chr5_grenenet_ann1.vcf.gz
bcftools tabix -f chr5_grenenet_ann1.vcf.gz 

bcftools query -l chr5_grenenet_ann1.vcf.gz | wc -l ## 900

bcftools merge --threads 4 --force-samples chr5_grenenet_ann1.vcf.gz chr5_grenenet_ann1.vcf.gz chr5_grenenet_ann1.vcf.gz chr5_grenenet_ann1.vcf.gz -o chr5_grenenet_ann2.vcf.gz
bcftools tabix -f chr5_grenenet_ann2.vcf.gz 

bcftools query -l chr5_grenenet_ann2.vcf.gz | wc -l ## 3600

bcftools merge --threads 4 --force-samples chr5_grenenet_ann2.vcf.gz chr5_grenenet_ann2.vcf.gz chr5_grenenet_ann2.vcf.gz chr5_grenenet_ann2.vcf.gz -o chr5_grenenet_ann3.vcf.gz
bcftools tabix -f chr5_grenenet_ann3.vcf.gz 

bcftools query -l chr5_grenenet_ann3.vcf.gz | wc -l ## 14400

bcftools merge --threads 4 --force-samples chr5_grenenet_ann3.vcf.gz chr5_grenenet_ann3.vcf.gz -o chr5_grenenet_ann4.vcf.gz
bcftools tabix -f chr5_grenenet_ann4.vcf.gz 

#for slim use, decompress
gunzip chr5_grenenet_ann4.vcf.gz 

#bcftools query -l chr5_grenenet_ann4.vcf.gz | wc -l ## 28800
# so this will give 2400 ind per subpop 

##### phase 5 #######################################
### run slim code thorugh all the optima ##########
#add permisison to run 
#chmod +x bash_slim.sh

echo PHASE 4 run slim simulation  
./bash_slim.sh

